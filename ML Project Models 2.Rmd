---
title: "Machine Learning Project"
author: ""
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#R Markdown

#Reading the Data:

```{r}
library(tidyverse)
rm(list=ls())

airline_data <- read.csv("./Data/AirlineCustomerSatisfaction.csv", header=T, stringsAsFactors=T)

summary(airline_data)
```


#Logistics Regression Model

Reasons to us the Logistic Regression Model: 

1. Generally easier to understand and interpret these models.
2. The model is faster to train than other models.
3. They can work well in scenerios in which the fit is not the best.

```{r}
summary(as.factor(airline_data$satisfaction))

```

#Fitting a logistic regression model 


```{r}
fit_1 <- glm(satisfaction ~., 
             family=binomial(link='logit'), 
             data= airline_data) 
summary(fit_1) 

```

All the variables in the model are significant.

```{r}
fit_2<- glm(Customer.Type ~., 
             family=binomial(link='logit'), 
             data= airline_data) 
summary(fit_2) 
```

#Significant Features 

```{r}

t1 <- summary(fit_1)
t1$coefficients[t1$coefficients[,4] < 0.001,]
```

```{r}
t2 <- summary(fit_2)
t2$coefficients[t1$coefficients[,4] < 0.001,]
```

Conclusion: All variables in the dataset are significant. None of them get eliminated in the above step.

#Categorical variables transformed into binary columns


#Now, data is ready to run models 

```{r}
satisfied <- as.numeric(airline_data$satisfaction)
customer_type <- as.numeric(airline_data$Customer.Type)
```


```{r}
model_data <- fastDummies::dummy_cols(airline_data[,c(2,4:23)], select_columns = c("Gender", "Type.of.Travel", "Class"),
   remove_selected_columns = TRUE)

model_data$satisfied <- satisfied
model_data$customer_type <- customer_type
```


```{r}
library(splitstackshape)
set.seed(123456) # Set seed
# Perform stratified sampling
split_dat <- stratified(model_data, # Set dataset
                       group = c("satisfied", "customer_type"), # Set variables to use for stratification
                       size = 0.2,  # Set size of test set
                       bothSets = TRUE ) # Return both training and test sets
# Extract train data
train_dat <- split_dat[[2]]
# Extract test data
test_dat <- split_dat[[1]]
```

```{r}
names(train_dat)
names(test_dat)

sum(is.na(train_dat))
sum(is.na(test_dat))

nlevels(factor(train_dat$satisfied))

summary(factor(train_dat$satisfied))
summary(factor(test_dat$satisfied))


```


```{r}
library(xgboost)
```

#### Satisfaction as response

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(train_dat[, c(1:25, 27)]), label = as.numeric(train_dat$satisfied) - 1)
# Create test matrix
dtest <- xgb.DMatrix(data = as.matrix(test_dat[, c(1:25, 27)]), label = as.numeric(test_dat$satisfied) - 1)
```

```{r}
set.seed(111111)
bst_1 <- xgboost(data = dtrain, # Set training data
               
               nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") 
```

#Xgboost Prediction Satisfaction

```{r}
library(xgboost)
library(caret)
library(OptimalCutpoints) # Load optimal cutpoints
library(ggplot2) # Load ggplot2
library(xgboostExplainer) # Load XGboost Explainer
library(pROC)
library(SHAPforxgboost)

set.seed(111111)
bst <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
               eta = 0.1, # Set learning rate
              
               nrounds = 500, # Set number of rounds
               early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
               verbose = 1, # 1 - Prints out fit
               nthread = 1, # Set number of parallel threads
               print_every_n = 20, # Prints out result every 20th iteration
              
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") # Set evaluation metric to use
```

```{r}
ggplot(bst$evaluation_log, aes(x = iter, y=test_error_mean))+
  geom_smooth()
```

#Tune the eta parameter for XGboost 


```{r}
set.seed(111111)
bst_mod_1 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, 
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") 
```

```{r}
set.seed(111111)
bst_mod_2 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth =  7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```


```{r}
set.seed(111111)
bst_mod_3 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10 , # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```

```{r}
set.seed(111111)
bst_mod_4 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.1, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") 
```


```{r}
set.seed(111111)
bst_mod_5 <- xgb.cv(data = dtrain, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```



#Extract and plot the variable importance for XGBoost 

```{r}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_error_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_error_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_error_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_error_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_error_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
```


```{r}
g_7 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7
```

From this it looks like an eta value of 0.3 gives the best results for this data set.


#Which features were most important for the Boost model? 

```{r}
set.seed(111111)
bst_final <- xgboost(data = dtrain, # Set training data
              
        
               
              eta = 0.3, # Set learning rate
              max.depth =  7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 100, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```


```{r}
# Extract importance
imp_mat <- xgb.importance(model = bst_final)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 10)
```


#ROC plot

```{r}
boost_preds <- predict(bst_1, dtest)

roc2 = roc(test_dat$satisfied, boost_preds)

plot.roc(roc2, print.auc = TRUE, col = "red", print.auc.col = "red")
```


The AUC for XG Boost is 0.994. 
AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.


```{r}
boost_preds <- predict(bst_final, dtest) # Create predictions for random forest model

# Convert predictions to classes, using 0.5
boost_pred_class <- rep("1", length(boost_preds))
boost_pred_class[boost_preds >= 0.5] <- "2"

t <- table(boost_pred_class, test_dat$satisfied) 
confusionMatrix(t, positive = "2")
```

```{r}
summary(as.factor(train_dat$satisfied))
```


________________________________________________________________________________________

#### Loyalty as response

```{r}
# Create training matrix
dtrain2 <- xgb.DMatrix(data = as.matrix(train_dat[, c(1:26)]), label = as.numeric(train_dat$customer_type) - 1)
# Create test matrix
dtest2 <- xgb.DMatrix(data = as.matrix(test_dat[, c(1:26)]), label = as.numeric(test_dat$customer_type) -1 )
```

```{r}
set.seed(111111)
# Set training data
bst_2 <- xgboost(data = dtrain2, # Set training data
               
               nrounds = 100, # Set number of rounds
               
               verbose = 1, # 1 - Prints out fit
                print_every_n = 20, # Prints out result every 20th iteration
               
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") 
```



#Xgboost Prediction Satisfaction

```{r}
library(xgboost)
library(caret)
library(OptimalCutpoints) # Load optimal cutpoints
library(ggplot2) # Load ggplot2
library(xgboostExplainer) # Load XGboost Explainer
library(pROC)
library(SHAPforxgboost)

set.seed(111111)
bst <- xgb.cv(data = dtrain2, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
               eta = 0.1, # Set learning rate
              
               nrounds = 500, # Set number of rounds
               early_stopping_rounds = 50, # Set number of rounds to stop at if there is no improvement
               
               verbose = 1, # 1 - Prints out fit
               nthread = 1, # Set number of parallel threads
               print_every_n = 20, # Prints out result every 20th iteration
              
               objective = "binary:logistic", # Set objective
               eval_metric = "auc",
               eval_metric = "error") # Set evaluation metric to use
```

```{r}
ggplot(bst$evaluation_log, aes(x = iter, y=test_error_mean))+
  geom_smooth()
```

#Tune the eta parameter for XGboost 


```{r}
set.seed(111111)
bst_mod_1 <- xgb.cv(data = dtrain2, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.3, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9, # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, 
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") 
```

```{r}
set.seed(111111)
bst_mod_2 <- xgb.cv(data = dtrain2, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.1, # Set learning rate
              max.depth =  7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```


```{r}
set.seed(111111)
bst_mod_3 <- xgb.cv(data = dtrain2, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.05, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10 , # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree =  0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```

```{r}
set.seed(111111)
bst_mod_4 <- xgb.cv(data = dtrain2, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.01, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0.1, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error") 
```


```{r}
set.seed(111111)
bst_mod_5 <- xgb.cv(data = dtrain2, # Set training data
              
              nfold = 5, # Use 5 fold cross-validation
               
              eta = 0.005, # Set learning rate
              max.depth = 7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample = 0.9 , # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 500, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
               
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```



#Extract and plot the variable importance for XGBoost 

```{r}
# Extract results for model with eta = 0.3
pd1 <- cbind.data.frame(bst_mod_1$evaluation_log[,c("iter", "test_error_mean")], rep(0.3, nrow(bst_mod_1$evaluation_log)))
names(pd1)[3] <- "eta"
# Extract results for model with eta = 0.1
pd2 <- cbind.data.frame(bst_mod_2$evaluation_log[,c("iter", "test_error_mean")], rep(0.1, nrow(bst_mod_2$evaluation_log)))
names(pd2)[3] <- "eta"
# Extract results for model with eta = 0.05
pd3 <- cbind.data.frame(bst_mod_3$evaluation_log[,c("iter", "test_error_mean")], rep(0.05, nrow(bst_mod_3$evaluation_log)))
names(pd3)[3] <- "eta"
# Extract results for model with eta = 0.01
pd4 <- cbind.data.frame(bst_mod_4$evaluation_log[,c("iter", "test_error_mean")], rep(0.01, nrow(bst_mod_4$evaluation_log)))
names(pd4)[3] <- "eta"
# Extract results for model with eta = 0.005
pd5 <- cbind.data.frame(bst_mod_5$evaluation_log[,c("iter", "test_error_mean")], rep(0.005, nrow(bst_mod_5$evaluation_log)))
names(pd5)[3] <- "eta"
# Join datasets
plot_data <- rbind.data.frame(pd1, pd2, pd3, pd4, pd5)
# Converty ETA to factor
plot_data$eta <- as.factor(plot_data$eta)
```


```{r}
g_7 <- ggplot(plot_data, aes(x = iter, y = test_error_mean, color = eta))+
  geom_smooth(alpha = 0.5) +
  theme_bw() + # Set theme
  theme(panel.grid.major = element_blank(), # Remove grid
        panel.grid.minor = element_blank(), # Remove grid
        panel.border = element_blank(), # Remove grid
        panel.background = element_blank()) + # Remove grid 
  labs(x = "Number of Trees", title = "Error Rate v Number of Trees",
       y = "Error Rate", color = "Learning \n Rate")  # Set labels
g_7
```


From this it looks like an eta value of 0.3 gives the best results for this data set.


#Which features were most important for the Boost model? 

```{r}
set.seed(111111)
bst_final <- xgboost(data = dtrain2, # Set training data
              
        
               
              eta = 0.3, # Set learning rate
              max.depth =  7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 100, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
               
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```


```{r}
# Extract importance
imp_mat <- xgb.importance(model = bst_final)
# Plot importance (top 10 variables)
xgb.plot.importance(imp_mat, top_n = 10)
```


Type.of.Travel_BusinessTravel and Age are most significant variables to predict loyalty.

#ROC plot

```{r}
boost_preds <- predict(bst_2, dtest2)

roc2 = roc(test_dat$customer_type, boost_preds)

plot.roc(roc2, print.auc = TRUE, col = "blue", print.auc.col = "blue")
```


AUC: 0.998  (Accuracy of model to predict customer loyalty)


```{r}
boost_preds <- predict(bst_final, dtest2) # Create predictions for random forest model

# Convert predictions to classes, using 0.5
boost_pred_class <- rep("1", length(boost_preds))
boost_pred_class[boost_preds >= 0.5] <- "2"

t <- table(boost_pred_class, test_dat$customer_type) 
confusionMatrix(t, positive = "2")
```


```{r}
summary(as.factor(train_dat$customer_type))
```


```{r}
zero_weight <- 19024/84880 
```


```{r}
set.seed(111111)
bst_bal <- xgboost(data = dtrain2, # Set training data
              
        
               
              eta = 0.3, # Set learning rate
              max.depth =  7, # Set max depth
              min_child_weight = 10, # Set minimum number of samples in node to split
              gamma = 0, # Set minimum loss reduction for split
              subsample =  0.9, # Set proportion of training data to use in tree
              colsample_bytree = 0.9, # Set number of variables to use in each tree
               
              nrounds = 200, # Set number of rounds
              early_stopping_rounds = 20, # Set number of rounds to stop at if there is no improvement
              scale_pos_weight = zero_weight,
              verbose = 1, # 1 - Prints out fit
              nthread = 1, # Set number of parallel threads
              print_every_n = 20, # Prints out result every 20th iteration
              
              objective = "binary:logistic", # Set objective
              eval_metric = "auc",
              eval_metric = "error")
```


```{r}
boost_preds_bal <- predict(bst_bal, dtest2) # Create predictions for XGBoost model

pred_dat <- cbind.data.frame(boost_preds_bal , test_dat$customer_type)
# Convert predictions to classes, using optimal cut-off
boost_pred_class <- rep("1", length(boost_preds_bal))
boost_pred_class[boost_preds_bal >= 0.5] <- "2"


t <- table(boost_pred_class, test_dat$satisfied) # Create table
confusionMatrix(t, positive = "2") # Produce confusion matrix
```

XG Boost End-







